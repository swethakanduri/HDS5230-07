{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "077d56be-150c-4886-bc97-a27364a1eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME: SWETHA KANDURI\n",
    "# WEEK 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a45aa38-d289-4cdd-89a7-1784cac16a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\n",
      "Pandas version: 2.2.2\n",
      "Matplotlib version: 3.8.4\n",
      "Numpy version: 1.26.4\n",
      "SciKitLearn version: 1.4.2\n",
      "My working directory:\n",
      "/Users/swetha\n",
      "My new working directory:\n",
      "/Users/swetha\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.85      0.81       970\n",
      "         1.0       0.66      0.52      0.58       530\n",
      "\n",
      "    accuracy                           0.73      1500\n",
      "   macro avg       0.71      0.68      0.69      1500\n",
      "weighted avg       0.73      0.73      0.72      1500\n",
      "\n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.85      0.80       970\n",
      "         1.0       0.65      0.52      0.58       530\n",
      "\n",
      "    accuracy                           0.73      1500\n",
      "   macro avg       0.71      0.68      0.69      1500\n",
      "weighted avg       0.72      0.73      0.72      1500\n",
      "\n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "Logistic_SL1_C_auto    0.7307   0.714 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "Logistic_SL1_C_auto    0.7307   0.714 \n",
      "RandomForest_noCV      0.9993   0.7   \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "Logistic_SL1_C_auto    0.7307   0.714 \n",
      "RandomForest_noCV      0.9993   0.7   \n",
      "RandomForest_CV        0.9953   0.704 \n",
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "Logistic_SL1_C_auto    0.7307   0.714 \n",
      "RandomForest_noCV      0.9993   0.7   \n",
      "RandomForest_CV        0.9953   0.704 \n",
      "RandomForest_CV2       0.7247   0.702 \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "%run Machine_Learning_Review_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1528aa3-924b-43b2-a91c-4f8c4d12611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7333   0.718 \n",
      "Null                   0.6467   0.608 \n",
      "Logistic_L1_C_1        0.732    0.716 \n",
      "Logistic_L1_C_01       0.726    0.706 \n",
      "Logistic_L1_C_10       0.7347   0.718 \n",
      "Logistic_L1_C_auto     0.7233   0.708 \n",
      "Logistic_SL1_C_auto    0.7307   0.714 \n",
      "RandomForest_noCV      0.9993   0.7   \n",
      "RandomForest_CV        0.9953   0.704 \n",
      "RandomForest_CV2       0.7247   0.702 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff3a9a1f-f49e-49ba-a262-0928b803626c",
   "metadata": {},
   "source": [
    "#1 Based on the performance metrics presented in the notebook, the **Logistic Regression model with L1 regularization and a higher regularization parameter (`Logistic_L1_C_10`) had the best overall performance. It achieved a training accuracy of 73.47% and a test (holdout) accuracy of 71.8%, which is the highest test accuracy among all the models evaluated. While the basic logistic regression model (`Logistic`) also achieved a test accuracy of 71.8%, the slightly higher training accuracy of `Logistic_L1_C_10` suggests better fit without significant overfitting.\n",
    "\n",
    "Other models like `RandomForest_noCV` and `RandomForest_CV` showed extremely high training accuracies (over 99%), but their lower test accuracies (70% and 70.4%, respectively) indicate signs of overfitting. The null model, which serves as a baseline, performed significantly worse with only 60.8% test accuracy. Therefore, considering both generalization performance and model stability, `Logistic_L1_C_10` offers the best trade-off between training accuracy and test accuracy, making it the top-performing model in this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7be0c66-8c16-4a32-aca5-bba9c0f4ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b8c7a73-8437-49f0-8370-ee3e1e60102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>...</th>\n",
       "      <th>Metastatic_solid_tumour</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>First_Appointment_Date</th>\n",
       "      <th>Last_Appointment_Date</th>\n",
       "      <th>DateOfDeath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962-02-27</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1959-08-18</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1946-02-15</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-07-27</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1983-02-19</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-09-22</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19996</td>\n",
       "      <td>1997-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-14</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19997</td>\n",
       "      <td>1984-03-31</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-04-24</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19998</td>\n",
       "      <td>1993-07-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19999</td>\n",
       "      <td>1984-04-17</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>20000</td>\n",
       "      <td>1966-05-14</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2012-05-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
       "0              1  1962-02-27  female  hispanic                      0   \n",
       "1              2  1959-08-18    male     white                      0   \n",
       "2              3  1946-02-15  female     white                      0   \n",
       "3              4  1979-07-27  female     white                      0   \n",
       "4              5  1983-02-19  female  hispanic                      0   \n",
       "...          ...         ...     ...       ...                    ...   \n",
       "19995      19996  1997-12-19  female     other                      0   \n",
       "19996      19997  1984-03-31  female     white                      0   \n",
       "19997      19998  1993-07-04  female     white                      0   \n",
       "19998      19999  1984-04-17    male     other                      0   \n",
       "19999      20000  1966-05-14  female     white                      0   \n",
       "\n",
       "       Congestive_heart_failure  Peripheral_vascular_disease  Stroke  \\\n",
       "0                             0                            0       0   \n",
       "1                             0                            0       0   \n",
       "2                             0                            0       0   \n",
       "3                             0                            0       0   \n",
       "4                             0                            0       0   \n",
       "...                         ...                          ...     ...   \n",
       "19995                         0                            0       0   \n",
       "19996                         0                            0       0   \n",
       "19997                         0                            0       0   \n",
       "19998                         0                            0       0   \n",
       "19999                         0                            0       0   \n",
       "\n",
       "       Dementia  Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  \\\n",
       "0             0          0  ...                        0    0        0   \n",
       "1             0          0  ...                        0    0        0   \n",
       "2             0          0  ...                        0    1        0   \n",
       "3             0          1  ...                        0    0        0   \n",
       "4             0          0  ...                        0    0        0   \n",
       "...         ...        ...  ...                      ...  ...      ...   \n",
       "19995         0          0  ...                        0    0        0   \n",
       "19996         0          0  ...                        0    1        0   \n",
       "19997         0          0  ...                        0    0        1   \n",
       "19998         0          0  ...                        0    0        0   \n",
       "19999         0          0  ...                        0    0        0   \n",
       "\n",
       "       Depression  Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
       "0               0             0      0        0              2013-04-27   \n",
       "1               0             1      0        0              2005-11-30   \n",
       "2               0             1      0        0              2011-11-05   \n",
       "3               0             0      0        0              2010-03-01   \n",
       "4               0             1      0        0              2006-09-22   \n",
       "...           ...           ...    ...      ...                     ...   \n",
       "19995           0             0      0        0              2008-06-14   \n",
       "19996           0             1      0        0              2007-04-24   \n",
       "19997           0             1      0        0              2010-10-16   \n",
       "19998           0             1      0        0              2015-01-04   \n",
       "19999           0             0      1        0              2011-04-01   \n",
       "\n",
       "       Last_Appointment_Date  DateOfDeath  \n",
       "0                 2018-06-01          NaN  \n",
       "1                 2008-11-02   2008-11-02  \n",
       "2                 2015-11-13          NaN  \n",
       "3                 2016-01-17   2016-01-17  \n",
       "4                 2018-06-01          NaN  \n",
       "...                      ...          ...  \n",
       "19995             2018-06-01          NaN  \n",
       "19996             2018-06-01          NaN  \n",
       "19997             2018-06-01          NaN  \n",
       "19998             2018-06-01          NaN  \n",
       "19999             2012-05-16          NaN  \n",
       "\n",
       "[20000 rows x 29 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set print limits\n",
    "pd.options.display.max_rows = 10\n",
    "## Import Data\n",
    "df = pd.read_csv(\"PatientAnalyticFile.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "266fe028-9528-44ea-b017-83a75c58952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target: 1 if patient is deceased, else 0\n",
    "df['Deceased'] = df['DateOfDeath'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ab4c09d-fb07-4d5c-b099-8be235ca47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=[\n",
    "    'PatientID', 'DateOfBirth', 'First_Appointment_Date', \n",
    "    'Last_Appointment_Date', 'DateOfDeath'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbf3459d-e8cb-4a77-820a-0b3ece353cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables (Gender, Race)\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "786f5b87-683c-45a5-a393-b05e9187bf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Race', 'Myocardial_infarction', 'Congestive_heart_failure',\n",
       "       'Peripheral_vascular_disease', 'Stroke', 'Dementia', 'Pulmonary',\n",
       "       'Rheumatic', 'Peptic_ulcer_disease', 'LiverMild',\n",
       "       'Diabetes_without_complications', 'Diabetes_with_complications',\n",
       "       'Paralysis', 'Renal', 'Cancer', 'LiverSevere',\n",
       "       'Metastatic_solid_tumour', 'HIV', 'Obesity', 'Depression',\n",
       "       'Hypertension', 'Drugs', 'Alcohol', 'Deceased'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "810f4222-e712-4640-a858-b2f077a3b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target\n",
    "X = df.drop(columns='Deceased')\n",
    "y = df['Deceased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68ad7c18-eb47-4b4d-9ceb-0c8e834fabbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Solver Comparison:\n",
      "\n",
      "  Solver used  Training subset accuracy  Holdout subset accuracy  \\\n",
      "0   liblinear                    0.6604                   0.6545   \n",
      "1       lbfgs                    0.6606                   0.6552   \n",
      "2   newton-cg                    0.6604                   0.6545   \n",
      "3        saga                    0.6604                   0.6545   \n",
      "4         sag                    0.6604                   0.6545   \n",
      "\n",
      "   Time taken (seconds)  \n",
      "0                0.0292  \n",
      "1                0.1403  \n",
      "2                0.0206  \n",
      "3                0.0658  \n",
      "4                0.1338  \n"
     ]
    }
   ],
   "source": [
    "# Split into 80% training, 20% holdout \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define solvers to test\n",
    "solvers = ['liblinear', 'lbfgs', 'newton-cg', 'saga', 'sag']\n",
    "results = []\n",
    "\n",
    "# Fit logistic regression models using each solver\n",
    "for solver in solvers:\n",
    "    if solver == 'liblinear':\n",
    "        model = LogisticRegression(solver=solver, penalty='l2', max_iter=1000)\n",
    "    else:\n",
    "        model = LogisticRegression(solver=solver, penalty=None, max_iter=1000)\n",
    "\n",
    "    # Time the training\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute accuracy\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Solver used\": solver,\n",
    "        \"Training subset accuracy\": round(train_acc, 4),\n",
    "        \"Holdout subset accuracy\": round(test_acc, 4),\n",
    "        \"Time taken (seconds)\": round(end - start, 4)\n",
    "    })\n",
    "\n",
    "# Create and display results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nLogistic Regression Solver Comparison:\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd3b0ea9-66b3-49de-8882-73195c2f5751",
   "metadata": {},
   "source": [
    "Based on the results from the logistic regression solver comparison, the solver that yielded the best overall performance was `liblinear`. All solvers achieved very similar training and holdout accuracies, with training accuracy hovering around 66.04% and holdout (test) accuracy ranging between 65.45% and 65.52%. While the `lbfgs` solver showed the highest test accuracy at 65.52%, this was only marginally better—by just 0.07%—than the accuracy achieved by `liblinear`, `newton-cg`, `saga`, and `sag` (all at 65.45%). However, when factoring in execution time, `liblinear` clearly outperformed the others, completing in just 0.0292 seconds, making it significantly faster than `lbfgs` (0.1403 seconds) and others like `sag` and `saga`. Given the negligible difference in accuracy but a significant improvement in computational efficiency, `liblinear` offers the best trade-off between accuracy and speed, making it the most effective solver overall in this comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
