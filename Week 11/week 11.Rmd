---
title: "R Notebook"
output: html_notebook
Author: Swetha kanduri
---

```{r}
library(readr)

# Load PimaIndiansDiabetes2 and prepare logistic model 
library(mlbench)
library(purrr)
data("PimaIndiansDiabetes2")
ds <- as.data.frame(na.omit(PimaIndiansDiabetes2))

logmodel <- glm(diabetes ~ ., data = ds, family = "binomial")
cfs <- coefficients(logmodel)
prednames <- variable.names(ds)[-9]

# Function to create a dataset
generate_dataset <- function(sz) {
  dfdata <- map_dfc(prednames,
                    function(nm){
                      eval(parse(text = paste0("sample(ds$", nm,
                                               ", size = sz, replace = T)")))
                    })
  names(dfdata) <- prednames
  
  pvec <- map((1:8),
              function(pnum){
                cfs[pnum+1] * eval(parse(text = paste0("dfdata$",
                                                       prednames[pnum])))
              }) %>%
    reduce(`+`) + cfs[1]
  
  dfdata$outcome <- ifelse(1/(1 + exp(-(pvec))) > 0.5, 1, 0)
  
  return(dfdata)
}

# Sizes you need
sizes <- c(100, 1000, 10000, 100000, 1000000, 10000000)

# Generate and save each dataset
for (sz in sizes) {
  df <- generate_dataset(sz)
  write_csv(df, paste0("synthetic_data_", sz, ".csv"))
}

```


```{r}
# load necessary libraries
library(xgboost)
library(readr)
# define dataset sizes to be processed
sizes <- c(100, 1000, 10000, 100000, 1000000, 10000000)

#Initialize results dataframe to store method, size, performance, and time
results <- data.frame(Method_used=character(),
                      Dataset_size=integer(),
                      Testing_set_predictive_performance=double(),
                      Time_taken_for_model_fit=double(),
                      stringsAsFactors=FALSE)
#Loop through each dataset size
for (sz in sizes) {
  cat("Processing dataset size:", sz, "\n")
 # read the synthetic dataset 
  df <- read_csv(paste0("synthetic_data_", sz, ".csv"))
  # feature x and y labels
  X <- as.matrix(df[, -ncol(df)])
  y <- as.numeric(df$outcome)
  
  # Train/test split
  set.seed(42)
  train_idx <- sample(seq_len(nrow(X)), size = 0.8 * nrow(X))
  
  X_train <- X[train_idx, ]
  y_train <- y[train_idx]
  X_test <- X[-train_idx, ]
  y_test <- y[-train_idx]
  # prepare Dmatrix objects for XGBoost
  dtrain <- xgb.DMatrix(data = X_train, label = y_train)
  dtest <- xgb.DMatrix(data = X_test, label = y_test)
  
  start_time <- Sys.time()
  
  param <- list(objective = "binary:logistic", eval_metric = "error")
  #Define parameters and train the model
  xgb_model <- xgb.train(params = param, data = dtrain, nrounds = 50, verbose = 0)
  
  end_time <- Sys.time()
  elapsed_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  preds <- predict(xgb_model, dtest)
  preds_binary <- ifelse(preds > 0.5, 1, 0)
  test_accuracy <- mean(preds_binary == y_test)
  # store the results
  results <- rbind(results,
                   data.frame(Method_used="XGBoost in R via direct xgboost() simple CV",
                              Dataset_size=sz,
                              Testing_set_predictive_performance=round(test_accuracy,4),
                              Time_taken_for_model_fit=round(elapsed_time,2)))
}

print(results)

# Save
write_csv(results, "r_xgboost_direct_results.csv")

```

```{r}
#Load necessary libraries
library(caret)
library(readr)
library(xgboost)
#Define dataset sizes to be processed
sizes <- c(100, 1000, 10000, 100000, 1000000, 10000000)
#nitialize results dataframe to store method, size, performance, and time
results_caret <- data.frame(Method_used=character(),
                            Dataset_size=integer(),
                            Testing_set_predictive_performance=double(),
                            Time_taken_for_model_fit=double(),
                            stringsAsFactors=FALSE)
#Loop through each dataset size
for (sz in sizes) {
  cat("Processing dataset size:", sz, "\n")
# read synthetic dataset
  df <- read_csv(paste0("synthetic_data_", sz, ".csv"))
#feature x and y labels
  X <- df[, -ncol(df)]
  y <- as.factor(df$outcome)

  # Train/test split
  set.seed(42)
  train_idx <- sample(seq_len(nrow(X)), size = 0.8 * nrow(X))

  X_train <- X[train_idx, ]
  y_train <- y[train_idx]
  X_test <- X[-train_idx, ]
  y_test <- y[-train_idx]

  train_data <- cbind(X_train, outcome = y_train)
#Set up cross-validation control with 5 folds
  control <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

  start_time <- Sys.time()
#Train the model using caret and XGBoost
  model <- train(outcome ~ ., data = train_data,
                 method = "xgbTree",
                 trControl = control,
                 tuneGrid = expand.grid(
                   nrounds = 20,            
                   max_depth = 6,
                   eta = 0.2,                
                   gamma = 0,
                   colsample_bytree = 0.8,    
                   min_child_weight = 1,
                   subsample = 0.8            
                 ),
                 verbose = FALSE)

  end_time <- Sys.time()
  elapsed_time <- as.numeric(difftime(end_time, start_time, units = "secs"))

  preds <- predict(model, newdata = X_test)
  test_accuracy <- mean(preds == y_test)
# store the results
  results_caret <- rbind(results_caret,
                         data.frame(Method_used="XGBoost in R via caret with 5-fold CV",
                                    Dataset_size=sz,
                                    Testing_set_predictive_performance=round(test_accuracy,4),
                                    Time_taken_for_model_fit=round(elapsed_time,2)))
}

print(results_caret)

# Save
write_csv(results_caret, "r_xgboost_caret_results_optimized.csv")

```

